---
title: "Evaluation of different, process statistic based approaches for describing and visualising measurement similarity in LC-HRMS nontarget measurement series as part of the qAlgorithms project"
author: "Daniel Höhn"
date: "`r Sys.Date()`"
output: 
bookdown::html_document2: 
  fig_caption: yes
  number_sections: yes
---
Report concluding the research practical by Daniel Höhn, supervised by Felix Drees and Gerrit Renner {.unnumbered}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggfortify)
library(plotly)
library(bookdown)
```

```{r for all datasets, include=FALSE}
allProcessed = read.csv("../log_qBinning_beta.csv", comment.char = "#")


allProcessed = as.data.frame(allProcessed[,2:(length(allProcessed[1,]))], row.names = allProcessed[,1])
# normalise errors of peakfinding to number of features
allProcessed$badFeatures = allProcessed$badFeatures / allProcessed$numFeatures

allProcessed$polarity = str_ends(row.names(allProcessed), "positive")
allProcessed$year = str_extract(row.names(allProcessed) , "^......")
allProcessed$year = as.numeric(allProcessed$year)
negatives = which(!allProcessed$polarity)
allProcessed$year[negatives] = allProcessed$year[negatives] * -1

allProcessed$blank = (str_detect(row.names(allProcessed), "H2O") | str_detect(row.names(allProcessed), "Blank"))
allProcessed$Series = "none"
allProcessed$Series[which(allProcessed$year == 202401)] = "PFAS+"
allProcessed$Series[which(allProcessed$year == -202401)] = "PFAS-"
allProcessed$Series[which(allProcessed$year == 202402)] = "PFAS+"
allProcessed$Series[which(allProcessed$year == -202402)] = "PFAS-"
allProcessed$Series[which(allProcessed$year == 202408)] = "pump_error+"
allProcessed$Series[which(allProcessed$year == 210229)] = "Aquaflow_D1+"
allProcessed$Series[which(allProcessed$year == 210229 & allProcessed$blank)] = "Aquaflow_D1_B+"
allProcessed$Series[which(allProcessed$year == 210311)] = "Aquaflow_D2+"
allProcessed$Series[which(allProcessed$year == 220909)] = "Indigo+"
allProcessed$Series[which(allProcessed$year == -220909)] = "Indigo-"
allProcessed$Series[which(allProcessed$year == 240123)] = "SFC_DoE+"
allProcessed$Series[which(allProcessed$year == -240123)] = "SFC_DoE-"
allProcessed$Series[which(allProcessed$year == 240821)] = "stds_deca+"
allProcessed$name = str_replace(str_sub(row.names(allProcessed), 7, -8), "pos_p", "p")
allProcessed$name = str_replace(str_replace(allProcessed$name, "^_", ""), "neg_n", "n")
allProcessed$name = str_replace(allProcessed$name, "05_AA_DK_Ibu", "PE")
allProcessed$name = str_replace(allProcessed$name, "Kali_Pu_Penc_2.9", "kali")

allProcessed = allProcessed[order(allProcessed$Series, allProcessed$name),]

numericCols = c(2,4,5,6,8)
allProcessed[,numericCols] = allProcessed[,numericCols]/allProcessed$numSpectra

pca_all = prcomp(allProcessed[, c(2:8,10)], scale = T )
pca_all$rotation = pca_all$rotation * -1

# [which(allProcessed$year == '21'),]

# pca_all$sdev^2 / sum(pca_all$sdev^2)

fullPCA = autoplot(pca_all, data = allProcessed[,], label = F, 
              label.size = 3.2, colour = 'Series', shape = T,

              loadings = TRUE, loadings.colour = 'blue',

              loadings.label = TRUE, loadings.label.size = 3)

# ggplotly(fullPCA)

### only use these for related data so the scaling stays ok ###
# mark breaks between Series in graphs
relativeSD = sd(allProcessed$numCentroids[130:139]) / mean(allProcessed$numCentroids[130:139])
DQSC_SD = sd(allProcessed$meanDQSC[130:139])
sd_bin1 = sd(allProcessed$numBins_one[130:139])
sd_bin0 = sd(allProcessed$numBins_empty[130:139])
sd_bin2 = sd(allProcessed$numBins_more[130:139])
sd_binAll = sd(allProcessed$numBins_more[130:139] + allProcessed$numBins_one[130:139])

sel = which(allProcessed$Series != "pump_error+")
model = lm(allProcessed$numFeatures[sel] ~ with(allProcessed, numBins_empty[sel] + numBins_one[sel] + numBins_more[sel]))

allProcessed$Series[c(93:102)] = "grad_pump_error+"

devline_all_nameless = ggplot(allProcessed)+
  geom_point(aes(x = (numBins_empty + numBins_one + numBins_more), y = numFeatures, colour = Series))+
  geom_abline(slope = model$coefficients[[2]], intercept = model$coefficients[[1]])+
  xlab("Relative Bin Count [Bins / Scan]")+
  ylab("Relative Feature Count [Features / Scan]")+
  labs(caption = paste0("R² = ", round(summary(model)$adj.r.squared, digits = 3)))

devline_all = ggplot(allProcessed)+
  geom_text(aes(x = (numBins_empty + numBins_one + numBins_more), y = numFeatures, label = name), size = 3.2)+
  geom_point(aes(x = (numBins_empty + numBins_one + numBins_more), y = numFeatures, colour = Series))+
  geom_abline(slope = model$coefficients[[2]], intercept = model$coefficients[[1]])+
  xlab("Relative Bin Count [Bins / Scan]")+
  ylab("Relative Feature Count [Features / Scan]")+
  labs(caption = paste0("R² = ", round(summary(model)$adj.r.squared, digits = 3)))

plot_series = list(Aquaflow = c(1:44, 130:139), SFC_DoE = c(118:129, 130:139), pump_error = 
                     c(103:117, 130:139), PFAS = c(76:92, 130:139), Kali_Ozone = c(45:75, 130:139))

```

## Table of Contents: {.unnumbered}
- [Abstract](#abstract)
- [Abbreviations and Definitions:](#abbreviations-and-definitions)
- [Process Quality Estimators](#process-quality-estimators)
  - [Differentiating Process Quality from Result Quality](#differentiating-process-quality-from-result-quality)
  - [Measuring Process Quality](#measuring-process-quality)
- [Selected Process Statistics](#selected-process-statistics)
  - [Test Dataset](#test-dataset)
- [Developed Tests](#developed-tests)
- [Calculations not Covered by qAlgorithms](#calculations-not-covered-by-qalgorithms)
  - [Preformance Criteria - Fourier Transform:](#preformance-criteria---fourier-transform)
  - [Performance Criteria - File Conversion](#performance-criteria---file-conversion)
- [Performance Criteria - Centroiding:](#performance-criteria---centroiding)
  - [Centroid Quality Score (DQSC)](#centroid-quality-score-dqsc)
  - [Signal Point Retention](#signal-point-retention)
- [Performance Criteria - Binning:](#performance-criteria---binning)
  - [Bin Quality Score (DQSB)](#bin-quality-score-dqsb)
  - [Bin Property Tests:](#bin-property-tests)
    - [Two or More Centroids of the Same Scan](#two-or-more-centroids-of-the-same-scan)
    - [Unbinned Points Within the Critical Distance](#unbinned-points-within-the-critical-distance)
    - [Signs of a Halved Peak](#signs-of-a-halved-peak)
    - [Drastic Mass Shifts](#drastic-mass-shifts)
    - [Asymmetric Distribution of MZ](#asymmetric-distribution-of-mz)
    - [Maximum Intensity at Edge of Bin](#maximum-intensity-at-edge-of-bin)
- [Performance criteria - Peak Finding:](#performance-criteria---peak-finding)
  - [DQSF](#dqsf)
  - [Replicate Features](#replicate-features)
  - [Validity of the Identified Peaks](#validity-of-the-identified-peaks)
- [Development of Consistency Parameters from Process Statistics](#development-of-consistency-parameters-from-process-statistics)
  - [Consistency of the Centroiding Algorithm](#consistency-of-the-centroiding-algorithm)
  - [Consistency of the Binning Algorithm](#consistency-of-the-binning-algorithm)
  - [Consistency of the Feature-finding Algorithm](#consistency-of-the-feature-finding-algorithm)
  - [Result Differences Between Centroid and Binning / Feature Test](#result-differences-between-centroid-and-binning--feature-test)
  - [Combined Binning and Feature Test](#combined-binning-and-feature-test)
- [Applying the Developed Tests](#applying-the-developed-tests)
  - [Applied Test: Centroid Stability](#applied-test-centroid-stability)
  - [Applied Test: Bin Stability](#applied-test-bin-stability)
  - [Applied Test: Feature Stability](#applied-test-feature-stability)
  - [Applied Test: Feature-Bin Ratio](#applied-test-feature-bin-ratio)
- [Further Research](#further-research)
- [Conclusion](#conclusion)
- [Software and Data](#software-and-data)
  - [Software](#software)
  - [Data](#data)
- [References](#references)

# Abstract

Untargeted analysis using mass spectrometry generates large amounts of data
that are difficult to store, share and process in large quantities. To adress
this issue, information regarding the processing performed with qAlgorithms
is examined regarding its chemical informativeness. The property chosen as 
representative for chemical information in general is the similarity of 
measurements taken using the same or similar system conditions, where very
low similarity without an evident reason rooted in the sample is taken as
a reason to consider it non-comparable to the other samples.

Such an estimate is especially relevant when routine measurements of, for example, 
wastewater treatment plant effluent are taken to monitor the overall water quality. 
The decision of whether a system is stable enough for measurements to be comparable
is non-standardised and often depending on manual data review. The amount of
manhours needed drastically reduces the amount of measurements which can be
processed as part of a routine series, while introducing a subjective bias
into the type of discrepancies that would lead to the exclusion of a measurement.

The goal of this project is to find, describe and test different, dataset-wide
properties which do not depend on knowledge regarding specific features.
System stability is chosen as one central component of result comparability
which is independent of detailed information regarding the samples used. 
To this end, statistics generated within the qAlgorithms data evaluation 
pipeline are collected for multiple different measurement series. The process 
is designed to be fully automated, requiring no user input during data processing.

Three complimentary approaches, each covering one of centroiding, binning and 
feature creation, were developed and tested using seven different measurement
series. The resulting graphs allowed a visual distinction between different
groups of data, as well as the identification of potential outliers within
groups. These methods can be used for (limited) applications where the question
of broad sample similarity arises and demonstrate the general utility of the 
chosen process statistics. This puts a future research focus on deriving
insights from high-resolution mass spectra that is not related to specific
features or components, but rather their distribution and relations.

The utility of the developed tests could be demostrated using a dataset
where the pump pressure fluctuation gradually increased, with the
measurements becoming unusable after a certain point. Additionally,
a generally applicable model of result consistency is presented for the qAlgorithms workflow

# Abbreviations and Definitions:
* NTS - Non-Target Screening
* (HR)MS - (High Resolution) Mass Spectrometry 
* m/z, mz - Mass to Charge Ratio
* RT - Retention Time
* GC - Gas Chromatography
* (HP)LC - (High Performance) Liquid Chromatography
* EIC - Extracted Ion Chromatogram
* DQS(B/C/F) - Data Quality Score (of a Bin/Centroid/Feature)
* ppm - Parts Per Million (10e-6)
* PCA - Principal Component Analysis

# Process Quality Estimators
An emerging and highly relevant problem in the field of NTS is
the fast and accurate estimation of total data quality or total
measurement quality through an operator. Especially for routine 
measurements, replicates and other series of very similar samples
this involves comparing the different steps of a series with 
each other, ideally over a large amount of samples at once.
The higher the amount of routine meansurements per sampling
interval, the higher the time needed for evaluation. This limits
the granularity of data and potentially restricts the analysis 
throughput to a number of measurements below the upper maximum
dictated through the instrumentation and selected chromatographic
method. Even if existing procedures need to be kept in place due
to contractual obligations, automated classification could select
the most relevant or representative measurement from a series of technical replicates.

Another point of application is the stability of a pooled sample
over the course of very long measurement series. Here, an easy way
of assessing the total stability while the instrument is still in 
operation allows the operator to react quickly to urgent problems
and gives limited insight into regions of the dataset not covered
by a more detailed stablility assessment using internal standards or 
the intensity of selected peaks chosen from a previously recorded
measurement of the pooled sample. Similar problems arise when
modifying the instrumentation while still requiring comparability
with the previous series.

These requirements place significant restrictions on the possible
ways to communicate the deluge of data that comes with (possibly)
years of measurements. Here, two types of approaches can be viable:
1) Machine-assisted or machine-led processing and search for regions of high change
2) A reductive approach that produces metadata which is still sensitive to shifts, but human-comprehensive

While these options are not exclusive, the final step of any
automated evaluation still needs to be that of communicating relevant
results to the instrument operator, ideally filtering out all irrelevant
information in the process. Such communication is especially
relevant to the qAlgorithms project, since the quality parameters supplied
can cause confusion with users.
For example, one user of the project asked which threshold to set
for selecting peaks generated with qAlgorithms. This misunderstanding
could be prevented by including an explicit descriptor that states
the viability of the measurement and communicates a point up to
which all data generated is reliable.
Similarly, (final) output data should always be provided in a format that
is compatible with spreadsheet software like Microsoft Excel, or,
for more complex data, some relevant subset which is human-readable. 

## Differentiating Process Quality from Result Quality
Previous work by Schulze et. al. (https://doi.org/10.1016/j.trac.2020.116063) highlighted
quality assurance as a core issue for nontargeted mass spectrometry and
discussed different dimensions for quality during an analysis.
Expanding on the step of "result reporting", a distinction should be
made between reporting the entire measurement for purposes of replication
and controlling through peer review and the reporting of just the 
measurement results. Schulze et. al. only consider reporting of the
results after interpretation, with the processing parameters and (unspecified)
statistical parameters being provided in addition. 
Confidence is also understood as the confidence of a correct identification,
not confidence in the peak being distinct from noise.
Besides it being impractical to replicate without all raw data being provided 
and thus very unlikely for this vital control to take place during peer 
review, it is not possible to estimate the algorithmic performance 
relating to result quality from just the algorithm settings on a per-sample 
basis accurately during NTA.

Result quality, as understood here, is the reliability and completeness
of all features at the end of any given processing workflow. For a
feature to be reliable, it has to correspond to a real molecule
that can be found within the sample in terms of mass and behaviour
in the chromatographic dimension or other dimensions of separation.
Furtermore, a reliable feature must conform to the MS2 spectrum,
if such a spectrum was produced by the instrument. After componentisation,
every component should include only features which have a high degree of 
certainty. The highest reliability in
a dataset is achieved if no false positives or negatives exist,
meaning the final results are a complete description of the sample contents.

Since this measure depends on ground truth, it is not always sensible 
to provide an estimator for result quality. The central difference 
between results and process as proposed here is that results serve
as the basis for some decision, be it through a researcher deciding
on the next experiment or a water treatment plant worker taking preventative
measures due to a detected contaminant. In all such cases, the process does not factor into
the final decision, unless it is as an abstract measure of confidence.
In contrast, the only decision based on a process parameter would be whether to
repeat a measurement due to a suspected error during sampling or analysis.

Process quality consists of the data processing workflow and the
(inferred) quality of the different steps from sampling to
retrieving the final result data set. The process can be split
into the two general steps of data acquisition and data processing.
The former presents a complex system that is impossible to perfectly
reproduce and very difficult to monitor in a quantitative manner.
While practices can be standardised, identification of errors 
with robust statistical methods is not easily realised once
sample processing has been performed, especially after prolonged periods of time. 
Here, error prevention is the most sensible practice, strategies for which have
been discussed by Schulze et. Al [https://doi.org/10.1016/j.trac.2020.116063] among others.
On the other hand, data processing must be deterministic
and fully quantifiable in order to enable automation. As such,
it is a viable point for introducing measures of process quality
both into existing systems and retroactively assessing the process
quality of past measurement series, provided the unmodified data was archived.

In terms of the fully formulated mathematical operations from
unprocessed data to the result table, quality describes the 
overlap between the assumed model that dictates selection,
combination and normalisation of data and the produced 
results. An example for such a quality parameter was developed
by Reuschenbach et al. [qcentroids]: The quality score describes
the overlap between a fitted gaussian curve and the measured 
data points. While such parameters give insight into the 
reliability of data, be it on a per-EIC, per-feature
or per-component level, this information does not directly 
derive from or relate to the result quality.

More importantly, if multiple steps feature a parameter of 
process quality, the different qualities likely can not
be converted into each other, or otherwise combined, without
losing relevant information or producing distorted estimations.

Under these conditions, deriving any indication of quantitative
result certainty on a per-result basis from the entire process 
is not possible. Instead, process quality can either serve
as a trigger for computation intensive corrections while
processing is active or as a way to provide non-quantitative
insight into the programs doing the processing to the operator.
Such insights are not limited to result certainty estimators,
but can give concrete feedback regarding the effectiveness
of a change in the process or the impact of a known disruption.

## Measuring Process Quality
Performance criteria must be robust, especially if further processing
actions are taken based on them. As such, established tests must
be validated using a representative sample of errors in the analysis
process. Current techniques to this end rely on a large amount and variety
of standard substances, which can then be described in terms
of recovery and mass accuracy of internal standards. [https://www.doi.org/10.1016/j.aca.2016.06.030]

Caballero-Casero et Al. [https://www.sciencedirect.com/science/article/pii/S0165993621000236] proposed guidelines on analytical
performance reporting in the context of human metabolomics.
The suggested criteria are sensitivity, selectivity, 
accuracy, stability and representativeness. All five largely
depend on internal standards, with the excpection of stability, which
also proposes consistency in the number of features as an addition 
to the recovery of internal standards.
The authors propose hard limit values for these, based on parameters
reported by questioned labs. These parameters 
In addition, internal standards never achieve full coverage
of a given chemical space in terms of retention time, mass
or ionisation behaviour. This limits the general applicability
of quality control by means of (exclusively) internal standards.
If very high amounts of standards are used in high concentrations
to facilitate high recovery, this also has the potential to mask
relevant trace compounds with similar behaviour.

# Selected Process Statistics
For the purposes of this analysis, characterisitic properties of
the qAlgorithms process were selected. These parameters are supposed
to cover the entire runtime of the algorithm and be significantly
different between different datasets. An advantage of only considering
process statistics is that the amount of data to be considered
is massively reduced, which in turn removes limits imposed by data storage
and transportation for almost all applications. Furthermore, if meaningful
information can be derived from such broad measures, this could sidestep
the non-technical restrictions imposed on sharing data by industry and government.

When assessing the entire process from profile-mode spectra to 
the final feature list, it is important to find parameters that 
are responsive to a broad number of deviations within the system,
while still being specific enough so that no change prevents another
from being percieved. An additional desireable quality is the tracability
from one detected error to the root cause or, in lieu of that, the point
in the algorithm where such an error occured.

Tested parameters:
* mean DQSC
* mean DQSB
* mean DQSF
* Number of centroids
* Number of bins
  * Bins with no features
  * Bins with one feature
  * Bins with more than one features
* Number of Features
  * Normal Features
  * Features with highly divergent mass trace

The three quality parameters (DQSC, DQSB, DQSF) calculated during the
different steps of qAlgorithms describe the model consistency of that
step to a limited degree. When taking their mean value at the end of their
respective step, they can indicate large differences between datasets
and make a statement about when during processing such a shift took place.
Here, an increase or decrease in quality is not necessarily related to an
actual increase in process quality for DQSC and DQSB. With the current
peak model, some noise tends to result in very low-intensity centroids 
with a very high score. Real signals have generally lower scores.
Similarly, bins with a real peak tend to have worse scores than very
small bins of data which does not contain a peak.
The DQSF is the only score which makes a statement about result quality,
since it serves as a measure of repeatability. [qPeaks]

The means of DQSC and DQSB were calculated from all produced scores. 
An alternative would be to only consider the scores of those 
points that were included in a feature, since both exist
as centroid-specific numbers at one point of the processing. This was 
decided against to mitigate the impact potential unknown problems with 
the processing would have on such a selection. A complimentary approach
which only considers such "useable" data should also incorporate the
as of now not implemented processes of componentisation and alignment.

The different counts were selected since they are not calculation
intensive or dependent on mathematical transformations to obtain. The latter
was avoided due to the possibility of introducing additional uncertainty
or some unknown bias into the parameters.

The centroid count is taken for all centroids with at least five profile
points. Depending on further additions to qAlgorithms, the count of 
centroids which do not fulfill this criteria in isolation, but are still
part of some regressions, could also be considered.

The three different bin counts were selected with the reasoning that 
many bins with no features are indicative of a noisy dataset and having
multiple peaks in the same bin implies poor chromatographic performance.
One bin containing one peak is the desired outcome.

For feature count and mean DQSF, only those features which do not 
fulfill the error condition were used. This is because the mechanism 
behind "unstable" mass traces is not understood to the extent that certainty
regarding their validity exist. The count of error-laden features is
not used as a process statistic, since it is unclear whether they represent
an oversight in the algorithm, a behaviour that can be corrected robustly
or an emergent property of the dataset not explained through other, more direct influences.

All three count statistics were normalised to the number of scans within a 
measurement to remain comparable between different measurement series.
This number changes within the individual series, but only by small amounts.
It could also be considered to remove those scans which are not shared between
datasets, provided a measure of analytical viablility on a 
per-spectrum basis is established.


## Test Dataset

The test dataset consists of eleven different groups from seven different
measurement series, all recorded using an orbitrap mass spectrometer and
electrospray ionisation, but differing chromatography and ion source conditions. For further details,
see [Software and Data](#software-and-data). Positive and negative mode 
measurements of the same sample are considered two groups, including 
one series using polarity switching.

The process statistics were controlled for normal distribution using a one-sided
t-test with an alpha of 0.01. No parameter was normally distributed. This is
taken as sufficient reason for them being meaningfully distinct between
datasets, under the assumption that algorithm-induced noise follows a
normal distribution. As such, all can be included in the PCA. 

Tests with different datasets showed that their correlation is inconsistent,
sometimes (anti)correlating strongly and other times being roughly orthogonal.
This implies that they contain information which can not be coerced into
one sum parameter without losing predictive power.

A PCA grouped by polarity was performed to find relevant parameters for 
process performance. In both cases, the numbers of centroids, bins and features
were highly correlating. In the positive case mean scores for bins and
features correlated, which did not apply for negative measurements. For both,
DQSC was roughly orthogonal to DQSF. When using only nine measurements from 
the same series, the relation and impact of the quality scores changed significantly.
One GC-HRMS dataset was also processed, but not included in the test dataset due to 
being too different from the LC-HRMS data and distorting the PCA. 

For further confirmation, one PCA containing all 139 processed samples was performed to similar results.

The PCA is provided as an interactive plot:
```{r Full_PCA, echo=FALSE, fig.cap = "PCA over 139 samples using the process statistics of normalised feature, bin and centroid counts and the three scores DQSC, DQSB, DQSF. The different counts correlate, while mean DQSB and DQSF roughly anticorrelate with the counts. the mean DQSC is orthogonal to both. Four separate groups can be observed for the pump error, all with large distance in the dominant principal component. Similarly, one outlier can be observed for the aquaflow D2 series. The ten replicates used as negative control cluster very closely together. The total coverage of 85% leaves the possibility for a relevant property to not influence this plot."}
ggplotly(fullPCA)
```

Due to the lack of established quality parameters, a comparison
with some "true" measure of quality is not possible. While the
quality scores implemented in qAlgorithms provide some information
regarding process quality, no correlation with result quality has
been established. Furthermore, errors persisting in the program
itself are possible. These factors reduce the viability of qAlgorithms
itself for controlling process quality variables. Since it could
be shown that the DQSF correlates with reproducibility [qPeaks],
The DQSF could function as an indicator for a process variable
not providing correct information should features with very data quality
scores contribute significantly to a very low process score.

To resolve this issue, two datasets - one for a case with no errors,
one for a case with a lot of errors - are used. The error-free series
consists of ten replicates of a matrix-free standard solution, measured 
in succession, with no noticeable deviation from each other. It is assumed
that this dataset and similar ones are fully free of data which would 
cause issues during processing with qAlgorithms. As such, any test that would
identify one of the ten measurements as defective is considered to be 
unreliable.

As a positive control, data from a UV degradation experiment was used.
During the measurement, the column pressure was highly unstable.
It is assumed that the individual measurements are both highly inconsistent
for replicates and generally contain bad peak assignments due to
the elution profile no longer being gaussian. A test which identifies
these measurements as defective is considered to be viable.

The mean DQSF values for the positive control average around 0.71,
while those for the standard solution average 0.73. With one exception,
the negative control measurements have the lowest DQSFs present in the
dataset. The four highest DQSFs in the test data belong to blank measurements
from two different series.

# Developed Tests

# Calculations not Covered by qAlgorithms 
// Diesen Abschnitt entfernen / kürzen? Ich halte es zwar für sinnvoll,
// auf die Schritte hinzuweisen, die wir nicht abdecken, bin mir mit der
// Umsetzung hier aber nicht sicher

## Preformance Criteria - Fourier Transform:
The fourier transform employed in FT-ICR and Orbitrap type mass
spectrometers is not something the instrument operator has access to.
With techniques like phase-constrained deconvolution [https://pubs.acs.org/doi/10.1021/acs.analchem.6b03636]
being possible, the potential negative influence of processing performed
in the instrument should be quantified and given with high granularity.
Without vendor cooperation, it will not be possible to estimate 
sensible quality criteria or to implement means of quality reporting
at this stage of the data processing.

## Performance Criteria - File Conversion
Due to most instruments providing measurements in proprietary formants,
they have to be converted using other software tools before processing.
At this stage, there is the potential for floating-point imprecision
to lead to different results for the same input file when using 
different software or different versions of the same software. [https://github.com/ProteoWizard/pwiz/issues/2407]
While the current accuracy of mass instruments does not usually
require more than 15 digits of precision, as the required accuracy
increases this could start affecting analysis results.
The maximum mass precision should be reported in the output file.
Conversion tools which depend on file transfer and user parameters
are additionally a potential source of user error. These cannot be
detected when using the vendor supplied instrument software for
visual inspection of the mass spectra.

# Performance Criteria - Centroiding:
It should be noted that not all approaches to mass spectrometric
data processing necesarily utilise centroiding of the raw spectra.
For these cases, measures of process quality are unlikely to be comparable 
to those presented here.
To differentiate between the data at different steps of processing,
a group of m/z value, retention time and intensity before any processing
has taken place is referred to as "signal point".

During centroiding, large amounts of signal points are reduced to
centroids, which encompass multiple signal points, while other signal
points are not added to centroids and consequently removed from the dataset.
A traditional estimation of false positive or false negatives is
not possible at this step, since any per-signal point information is 
lost during the following steps towards componentisation.

## Centroid Quality Score (DQSC)
The DQSC currently implemented through the qAlgorithms workflow
gives the percentage to which any given centroid conforms to the
expected gaussian shape. This correlates with the reliability
of the generated centroid. [qCentroids] 
The following test utilising the DQSC was considered:

For every measurement, the mean DQSC and the mean DQSCs of
upper and lower quartile were calculated with the compared 
criteria being the mean relative change between the three.
This test was discarded because the measure of difference was
smaller than 50% of the standard deviation of the DQSC for
all samples of the negative control series.

## Signal Point Retention

// Ich bin mir hier nicht sicher, wie sinnvoll ein entsprechender 
// Parameter tatsächlich ist. Müsste man wahrscheinlich ausprobieren.

Another perspective on process quality is the unifomity of intermediate
results when comparing replicates or time series that should be 
consistent, for example drinking water. In these cases, under 
ideal circumstances, all processing steps should have very similar
results, not just the final componentisation.
One such process parameters that can be obtained is the percentage
of signal points that are included in centroids, for every mass
spectrum in a measurement.

This parameter is not yet implemented in qAlgorithms at the time
of writing, while further questions as to the concrete implementation
remain. Notably, points at the edges of the observed mass region are
generally less relevant to the dataset, and the first few scans
encompass the void time of the chromatography. 
Since development of the centroiding algorithm is not fully completed,
future changes stand to enable this and similar measures with a more
represenatative coverage of the dataset than currently possible.

# Performance Criteria - Binning:

## Bin Quality Score (DQSB)
The DQSB in its current implementation [@todo] (paper binning, report 1) gives a measure of how well separated
a bin is from its environment by comparing the distances within a bin with
the distance to the closest point outside of it. In its original conception,
this only takes the retention time into account when searching for the closest
points not in the bin. To achieve greater locality of the score, the program
was modified such that only points within the maximal gap distance are used
to calculate the mean in-group differences in mz. This change led to, on
average, slightly lower scores. Furthermore, scaling was introduced for 
the outer distance (see [here](#dqsb-calculation)).

## Bin Property Tests:
// Es lohnt sich wahrscheinlich nicht, diesen Abschnitt beizubehalten. Ich halte die
// Tests zwar für erwähnenswert, aber sie tragen nicht wirklich zum Gesamtkunstwerk bei.
In addition to the quality score, multiple property tests are performed on 
each bin. These serve the purpose of descibing commonly observed patterns
during binning that often occur in distorted bins. These secondary tests
are used to take corrective action where this is possible, or to relax the
strictness of some binning parameters. This is sometimes necessary because
the peak finding algorithm assumes all centroids to have the same mz. 

While not all tests are applicable for improving the data passed to qPeaks, 
they give further insight into how the process functioned. Since all tests
aim to identify points of failure in the bins without knowledge about the
regression, the number of bins which did not fail any test, the number of
these bins that contained at least one peak and the number of bins with 
at least one positive test that contained one or more peaks are also taken
as parameters for the binning / peak finding process.

A complete overview of all currently used tests and those tests that were
implemented at one point and then removed is given below. Tests can be
used both for corrective action and to assess bin quality, since a correction
is not always possible. Every bin which shows positive for these could
not be improved through the described process.
As of now, the found peaks do not stem from any bins that did not trigger 
any test during binning. The additional criteria are not considered to
carry any degree of certainty for that reason.

### Two or More Centroids of the Same Scan
This test is necessary because the peakfinder cannot utilise more than one 
centroid per scan. All bins where this applies are marked. Before finding
peaks, only the centroid with the highest DQSC is retained. For most 
datasets, the rate of bins with duplicates is below 0.1%, with many
affected bins being ones that contain a hundred or more centroids.

### Unbinned Points Within the Critical Distance 
During binning, sometimes points are not included in a bin
despite - taken in isolation - fulfilling the criteria for
inclusion. However, they cannot be part of the bin without 
also including more points. Since the critical distance
shrinks with the number of points, the bin is split
into the one or two bins that fulfill this test.
This test does not count points that would increase
the critical distance such that the bin would be split
by themselves.

This test is used in conjunction with the next to 
combine two bins which show signs of being split in
the middle of a peak.

### Signs of a Halved Peak
If a peak shows a uniform monotonous rise in intensities,
which is defined as at least every other centroid having 
a higher intensity than the previously highest intensity,
it is considered to be one half of a peak. Bins which 
fulfill this criteria generally contain few data points.

Both this and the previous test are specific to the beginning
and end of a peak, although this information is lost after 
completed binning.

In some cases, peaks that are part of one very large bin
are cut, despite the mass differences being within tolerance
if they are viewed without considering the large amount of noise
data that is also within the bin. If the other half ends up as a 
viable bin, one correct peak more can be found if both halves 
are merged again. Since they were split due to too many points
lowering the critical distance below what is reasonable for the
peak, if a bin that could contain half a peak has a point of another
bin in range (determined by the previous test), both bins have 
a distance of no more than three scans and the assumed peak apex is
in the same direction (in scans) as the bin that is considered for
a merge, Both are combined even if this leads to the critical distance
being surpassed. If this merge is correct, one more peak can be 
characterised. If it is incorrect, either no peak is found in the
region and the merge has no effect or, in the worst case, the mass
of a correct peak is slightly distorted by including some 
otherwise avoidable noise.

### Drastic Mass Shifts
A bin is considered to undergo a drastic mass shift if median and
mean in mz are more than two times the mean centroid error apart.
Bins display this behaviour if at the end of a bin, few points 
undergo a drastic increase or decrease in mz.
This test is not used for corrective action.

### Asymmetric Distribution of MZ
Assuming a normal distribution, 80% of points should be within the
1.3 sigma interval. 80% were chosen as the limit so that one point
of deviation is still within tolerance for bins of size 5.
This test is not used for corrective action.

### Maximum Intensity at Edge of Bin
If the highest intensity in a bin is at the first or last centroid of
the bin, it can not be the apex of an interpolated peak. This goes
against the model behaviour of a correct bin.
This test is not used for corrective action, but could be employed 
as another sign of a halved peak.
# Performance criteria - Peak Finding:
## DQSF
The DQSF describes how well the chromatographic peak underlying a 
feature fits the peak model. It already serves as a way for the user
to priorise peaks for further analysis and make a qualified decision 
regarding conflicts in componentisation or assignemnt of MS2 spectra. [qpeaks]
However, the feature-specific nature of the DQSF makes it unwieldy as
a tool for assessing the entire dataset. When taking the mean or median,
The few features of extremely high quality lose impact, whereas a more
selective subset of peaks loses its validity concerning the entire 
dataset. While the mean DQSF is used for sum parameter characterisation
of a dataset, it is not used as the foundation for a test regarding
consistency or anything else. Based on preliminary tests it is assumed 
that a relevant difference in DQSF does not provide information not 
covered by any of the other presented tests.

## Replicate Features
While not implemented at this point in time, the number of times
a feature is found in a set of technical replicates is also of 
interest for evaluating process quality. If process statistics 
can be shown to correlate with high replicability, this provides both
further insight into the importance different processing steps have
and a way to optimise instrument conditions towards good data
while reducing the measurements performed to this end.

## Validity of the Identified Peaks
During development, it was found that some generated peaks will not
fulfill the condition of being part of only one mass trace. Around two
percent of all peaks found displayed this behaviour. This property
of a peak implies poorly resolved mass traces during the binning step,
while the fact that a peak could be constructed points towards mostly singular 
noise datapoints being included. Peak scores in these "contaminated" bins are
not noticeably distinct from those in regular bins.
It is possible that these bad inclusions are a result of the "correct" centroid,
that being the real signal of this ion trace in the given scan, not
being detected by the centroiding algorith while still being measured in
the instrument. Such issues could be addressed by finding a way to utilise
regions of the scan which contain real peaks with a width less than 
five and as such not constituting a viable peak during centroiding
or by interpolating such points instead when fitting the regression.
It could also be possible to introduce more region specific filters during
binning to find these points and search for a better fitting centroid.
The total number of these peaks is also a process statistic that can be
used for generalised performance assessment, since every peak with these 
characteristics represents an error during processing.
It is, however, not directly clear where in processing this error occurred.
It could be that in the profile data, a better fitting signal was overlooked
because it did not fulfill the minimum standards for a peak. The possibility
exists that after binning, if the relevant point was a duplicate, the 
wrong choice was made. This cannot be the main reason for such errors due
to the rarity of duplicate scans in bins. It is also possible that no
profile signal exists at the point.

Another frequent cause of this behaviour is the centroid mass showing a
rapid change in either direction once the signal intensity surpasses 
some value. The mass returns to its previous position once the intensity
is below this value again. Examples of this effect could be observed in all
controlled samples, which makes the hypothesis that this is an orbitrap-specific
effect plausible. 

Corrective measures will have to differentiate between both cases.


# Development of Consistency Parameters from Process Statistics
## Consistency of the Centroiding Algorithm {.tabset}

It was observed that, when ordered by time of recording, the differences
between scores of neighbouring measurements are close to zero for defect-free
measurement series. Two different series of similar samples which were
measured one month apart had very similar DQSCs, barring one exception.
DQSB and DQSF showed a similar jump in difference, but tend to have a higher 
base fluctuation in the used datasets.
In the tested data, the last measurement of the continued series had 
abnormally many spectra recorded. This lead to two noticeable changes
in DQSC (refer image aquaflow). 

Similar effects can be observed when viewing the differences between
the total number of centroids per measurement. To display both graphs together,
they were normalised so that their greatest point, respectively, was 1.
The number of recorded spectra is inconsistent between measurements,
usually deviating by around four. Whether this had an influence on the
displayed results was tested by calculating the differences between
all points before and after normalising them to the number of recorded
spectra. Both series were scales afterwards such that their maximum value was one.
The greatest absolute difference between the two series was 1.1 10e-16.
This is within the floating point error, and both results can be
considered identical. The most visible difference to the DQSC variation
is that here, blanks do appear as a visible decrease.

Normalising to the number of spectra is done primarily to use the ten
replicate measurements as a baseline for series measured using another method.
In a real application, normalisation is not necessary.

The solution chosen for confidence limits uses a separately recorded
series of ten technical replicates. Their similarity is confirmed via
PCA before asserting them as the best-case scenario for a given set
of instrument conditions. In the shown series, this reference data
is included as a separate series using the naming SP_DDA... and
separated using a dashed black line.
The warning limit is set to ten times the greatest relative standard
deviation of the control series regarding scaled DQSC and centroid count,
the exception limit to twenty times that.
The exception limit was chosen such that the difference between different
measurement series always surpassed this value with at least one of
the two differences, while the warning limit was set to 50% of that.
The warning limit does not serve a clear purpose in its current implementation,
since all known outliers massively exceeded the exception limit. For
a better value, a measurement displaying relevant drift in this parameter is
required for a heuristic estimation of sensible limits.

The combined view shows a change in both parameters if the series changes,
both being largely constant within the group of known good measurements.
An increase in DQSC does not necessarily coincide with an increase in 
the centroid count. Within a series, some cases exist where only one of the
two experiences a major shift. Neither of the two has a consistently higher 
noise level. Black, dashed lines denote the border between two series.
At these points, it is expected to see a major shift in both score and
count once. This combination exists primarily to reduce the number of
images which constitute the final assessment and is not required for
an automated evaluation.

For an eventual user-facing implementation, blanks could be removed 
when displaying the differences in count and score. Additionally, the
display algorithm should account for outliers and add an explanation for 
the operator to the relevant region. This could also be realised by two 
independent tests for consistency on mean DQSC and centroid count.

```{r plot selection, echo=FALSE}
i = 1

for (series in plot_series) {
  selectedProcess = allProcessed[series, ]
  
SeriesBreaks = (selectedProcess$Series[-1] != selectedProcess$Series[-length(selectedProcess$Series)])
SeriesBreaks = which(SeriesBreaks) + 0.5


Sample = as.factor(1:(length(selectedProcess$numSpectra)-1)) 
label = selectedProcess$name[-length(Sample)]

s1 = max(abs(diff(selectedProcess$meanDQSC)))
s2 = max(abs(diff(selectedProcess$numCentroids)))

delim = 10 * max(relativeSD / s2, DQSC_SD / s1)
  
plot = ggplot()+
  geom_line(aes(Sample, (diff(selectedProcess$meanDQSC))/s1, group = "DQSC", colour = "DQSC"), linewidth = 1, alpha = 0.8)+
  geom_line(aes(Sample, diff(selectedProcess$numCentroids)/s2, group = "Count", colour = "Count"), linewidth = 0.85, alpha = 0.8)+
  geom_vline(xintercept = SeriesBreaks, linewidth = 0.6, linetype = "dashed", alpha = 0.5)+
  geom_hline(yintercept = c(delim, -delim), linewidth = 0.4, colour = "green3")+
  geom_hline(yintercept = c(delim * 2, -delim * 2), linewidth = 0.4, colour = "red")+
  scale_x_discrete(labels = label, guide = guide_axis(angle = 60))+
  xlab("Sample")+
  ylab("Centroid Dissimilarity")
  
assign(paste0("plot",i), plot, envir=.GlobalEnv)
  
i = i+1
if (i==2){
  stop()
}
}
```

### Dataset: Pump Error
```{r plot3, echo=FALSE}
print(plot3)
```

For the positive control, this visualisation is made hard to evaluate
visually due to the very large change in centroid count to the 
replicates. For better visibility, these are not included in the graph. (centroid_test_PC)
Despite one point over the warning limit and two others visibly
different from 0, with the current configuration it is not obvious
that this data is not sensible to consider for further analysis.

### Dataset: Aquaflow
```{r plot1, echo=FALSE}
print(plot1)
```

In the Aquaflow data, the previously known outlier can be identified by a 
large change in both parameters. The three blank measurements also
exceed the exception limit. While the centroid count stays largely 
constant, the DQSC varies enough to surpass the warning limit for
cases in which differing sampling points are next to each other.

### Dataset: SFC-DoE
```{r plot2, echo=FALSE}
print(plot2)
```
The SFC data shows much stronger deviation than the control series.
The change from positive to negative data causes the centroid count to
go over the exception limit, but not the DQSC. This behaviour is expected,
since the different points have differing settings for the ion source.

### Dataset: Calibration Series
```{r plot5, echo=FALSE}
print(plot5)
```
In the calibration series, negative measurements are more similar than 
positive ones. The change from positive to negative mode causes both 
parameters to exceed the exception limit. One possible outlier exists,
although it is not detected through one of the other tests.